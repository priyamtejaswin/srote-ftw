{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Super Resolution, On The Edge, For The Win!\r\n",
      "*by Akshay Chawla and Priyam Tejaswin*\r\n",
      "\r\n",
      "Main reference paper : [Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation, CVPR 2017](http://openaccess.thecvf.com/content_cvpr_2017/papers/Caballero_Real-Time_Video_Super-Resolution_CVPR_2017_paper.pdf)\r\n",
      "```\r\n",
      ".\r\n",
      "├── README.md\r\n",
      "├── papers ## PDFs for review and reference (not committed).\r\n",
      "├── src_tensorflow ## Code in tf (using `tensorflow.keras`). \r\n",
      "└── src_tf2 ## Code in tf2_alpha.\r\n",
      "```\r\n",
      "\r\n",
      "## Dependencies\r\n",
      "- `src_tensorflow` uses **tensorflow 1.13.1**. This is the latest stable release as of creating this repository.\r\n",
      "- `src_tf2` uses **tensorflow 2.0** which is still an alpha build.\r\n",
      "\r\n",
      "## System Design\r\n",
      "![](system_blocks.jpg)\r\n",
      "\r\n",
      "## References\r\n",
      "- VESPCN implementation (unofficial, TF) - <https://github.com/LoSealL/VideoSuperResolution/blob/master/VSR/Models/Vespcn.py>\r\n",
      "- Fully-featured TF2 implementation of YOLOv3 (for checking TF2 oddities) - <https://github.com/zzh8829/yolov3-tf2>"
     ]
    }
   ],
   "source": [
    "!cat ../README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "print tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(\"../README.md\")\n",
    "dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "dataset = dataset.filter(lambda row: tf.size(row) > 3)\n",
    "dataset = dataset.map(lambda row: tf.slice(row, [0], [2]))\n",
    "dataset = dataset.window(size=3, shift=1, drop_remainder=True) ## THIS WILL RETURN A DATASET!!!!\n",
    "dataset = dataset.flat_map(lambda dset: dset.batch(3))\n",
    "# How to shuffle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(# Super Resolution, On The Edge, For The Win!, shape=(), dtype=string)\n",
      "tf.Tensor(*by Akshay Chawla and Priyam Tejaswin*, shape=(), dtype=string)\n",
      "tf.Tensor(, shape=(), dtype=string)\n",
      "tf.Tensor(Main reference paper : [Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation, CVPR 2017](http://openaccess.thecvf.com/content_cvpr_2017/papers/Caballero_Real-Time_Video_Super-Resolution_CVPR_2017_paper.pdf), shape=(), dtype=string)\n",
      "tf.Tensor(```, shape=(), dtype=string)\n",
      "tf.Tensor(., shape=(), dtype=string)\n",
      "tf.Tensor(├── README.md, shape=(), dtype=string)\n",
      "tf.Tensor(├── papers ## PDFs for review and reference (not committed)., shape=(), dtype=string)\n",
      "tf.Tensor(├── src_tensorflow ## Code in tf (using `tensorflow.keras`). , shape=(), dtype=string)\n",
      "tf.Tensor(└── src_tf2 ## Code in tf2_alpha., shape=(), dtype=string)\n",
      "tf.Tensor(```, shape=(), dtype=string)\n",
      "tf.Tensor(, shape=(), dtype=string)\n",
      "tf.Tensor(## Dependencies, shape=(), dtype=string)\n",
      "tf.Tensor(- `src_tensorflow` uses **tensorflow 1.13.1**. This is the latest stable release as of creating this repository., shape=(), dtype=string)\n",
      "tf.Tensor(- `src_tf2` uses **tensorflow 2.0** which is still an alpha build., shape=(), dtype=string)\n",
      "tf.Tensor(, shape=(), dtype=string)\n",
      "tf.Tensor(## System Design, shape=(), dtype=string)\n",
      "tf.Tensor(![](system_blocks.jpg), shape=(), dtype=string)\n",
      "tf.Tensor(, shape=(), dtype=string)\n",
      "tf.Tensor(## References, shape=(), dtype=string)\n",
      "tf.Tensor(- VESPCN implementation (unofficial, TF) - <https://github.com/LoSealL/VideoSuperResolution/blob/master/VSR/Models/Vespcn.py>, shape=(), dtype=string)\n",
      "tf.Tensor(- Fully-featured TF2 implementation of YOLOv3 (for checking TF2 oddities) - <https://github.com/zzh8829/yolov3-tf2>, shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for batch in dataset:\n",
    "        print i, batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tejaswin.p/THIS_LAPTOP_projects/srote-ftw/src_tensorflow\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump_frames.py rough.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir('helloworld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rough.ipynb', 'dump_frames.py', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "\n",
    "VFDEL = '___' ## VideoName___FrameId delimiter.\n",
    "NFRAMES = 2 ## Number of frames considered per sample.\n",
    "\n",
    "def _fskey(f):\n",
    "    vname, fid = f.rstrip('.png').split(VFDEL)\n",
    "    fid = int(fid)\n",
    "    return vname, fid\n",
    "\n",
    "def build_dataset(fdir):\n",
    "    all_frames = []\n",
    "\n",
    "    vdirs = [os.path.join(fdir, d) for d in os.listdir(fdir)]\n",
    "    vdirs = [d for d in vdirs if os.path.isdir(d)]\n",
    "    print \"Found %d vid files.\"%len(vdirs)\n",
    "\n",
    "    for dpath in vdirs:\n",
    "        print \"In %s\"%dpath\n",
    "        frames = [f for f in os.listdir(dpath) if f.endswith('.png')]\n",
    "        print \"\\tFound %d frames.\"%len(frames)\n",
    "\n",
    "        frames = sorted(frames, key=lambda x:_fskey(x))\n",
    "        frames = [os.path.join(dpath, f) for f in frames]\n",
    "\n",
    "        ## Group sorted frames by NFRAMES.\n",
    "        motion_frames = []\n",
    "        for i in range(len(frames) - NFRAMES + 1):\n",
    "            motion_frames.append(tuple(frames[i : i+NFRAMES]))\n",
    "\n",
    "        ## Add to master list.\n",
    "        all_frames.extend(motion_frames)\n",
    "\n",
    "    print \"Found %d frame groups of length %d.\"%(len(all_frames), NFRAMES)\n",
    "    return all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 vid files.\n",
      "In ../data/frames/hall_monitor_cif.y4m\n",
      "\tFound 300 frames.\n",
      "In ../data/frames/sample_hd_video.mp4\n",
      "\tFound 376 frames.\n",
      "Found 674 frame groups of length 2.\n"
     ]
    }
   ],
   "source": [
    "batched_fnames = build_dataset('../data/frames/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 288, 352, 3)\n",
      "(1, 288, 352, 3)\n",
      "(1, 288, 352, 3)\n",
      "(1, 288, 352, 3)\n",
      "(1, 288, 352, 3)\n",
      "(1, 288, 352, 3)\n"
     ]
    }
   ],
   "source": [
    "## Using generator...\n",
    "# dataset = tf.data.Dataset.from_generator(lambda: batched_fnames[:10], (tf.string, tf.string))\n",
    "# dataset = dataset.shuffle(buffer_size=len(batched_fnames))\n",
    "# dataset = dataset.batch(3)\n",
    "\n",
    "## Using tensor_slices...\n",
    "dataset = tf.data.Dataset.from_tensor_slices(batched_fnames[:3])\n",
    "# dataset = dataset.shuffle(buffer_size=len(batched_fnames))\n",
    "dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices) ## Order will be preserved | https://stackoverflow.com/questions/51015918\n",
    "dataset = dataset.map(load_image)\n",
    "# dataset = dataset.map(process_image)\n",
    "\n",
    "for row in dataset:\n",
    "    print row.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  2  4  6  8 10]\n",
      " [12 14 16 18 20 22]\n",
      " [24 26 28 30 32 34]\n",
      " [36 38 40 42 44 46]\n",
      " [48 50 52 54 56 58]\n",
      " [60 62 64 66 68 70]]\n",
      "[[ 1  3  5  7  9 11]\n",
      " [13 15 17 19 21 23]\n",
      " [25 27 29 31 33 35]\n",
      " [37 39 41 43 45 47]\n",
      " [49 51 53 55 57 59]\n",
      " [61 63 65 67 69 71]]\n"
     ]
    }
   ],
   "source": [
    "img = np.arange(36 * 2).reshape([1, 6, 6, 2])\n",
    "print img[0, :, :, 0]\n",
    "print img[0, :, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  2  4  6]\n",
      " [12 14 16 18]\n",
      " [24 26 28 30]\n",
      " [36 38 40 42]]\n",
      "[[ 1  3  5  7]\n",
      " [13 15 17 19]\n",
      " [25 27 29 31]\n",
      " [37 39 41 43]]\n"
     ]
    }
   ],
   "source": [
    "patches = tf.image.extract_image_patches(img, ksizes=[1, 4, 4, 1], strides=[1, 2, 2, 1], rates=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "patches.numpy().shape\n",
    "print patches[0, 0, 0].numpy().reshape([4, 4, 2])[:, :, 0]\n",
    "print patches[0, 0, 0].numpy().reshape([4, 4, 2])[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(fpath):\n",
    "    \"\"\"\n",
    "    Load and 'snap' to compatible size (extract 96x96 grids).\n",
    "    \"\"\"\n",
    "    image_string = tf.read_file(fpath)\n",
    "    image = tf.image.decode_png(image_string, channels=3)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    patches = tf.image.extract_image_patches(\n",
    "        image,\n",
    "        ksizes = [1, 96, 96, 1],\n",
    "        strides = [1, 14, 14, 1],\n",
    "        rates = [1, 1, 1, 1],\n",
    "        padding = 'VALID'\n",
    "    )\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "a = range(25)\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(a):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
